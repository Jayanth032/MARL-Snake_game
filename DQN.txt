Deep Q Network

-> DQN is an extension of traditional Q-learning where we replace the Q-table with a neural network.
-> Instead of storing Q(s,a) values in a table, the network learns to approximate the Q-values for each action given a state.
-> This allows agents to handle large or continuous state spaces, where tabular methods (like Linear Q-learning) fail.

How We Used DQN in Our Snake Game
-> Each snake agent was trained using a DQN to learn optimal actions over time.

State Representation:
-> Relative position of food
-> Immediate danger in all directions
-> Current direction of movement

Actions:
-> Turn left, go straight, or turn right (relative to current direction)

Reward Function:
-> +10 for eating food
-> -10 for dying (collision with wall or snake)
-> -0.1 for each step (to encourage efficient paths)
